%!TEX root = main.tex

\section{Spark Driver-less} % (fold)
\label{sec:spark_driver_less}


\subsection{Background} % (fold)
\label{sub:background_driver_less}

The original Spark model is a centralized system.
Client send job to the driver, and driver will split into tasks and send to multiple workers.
When worker finishes the task, it will send it back to driver.
If any worker is failed, the driver will notice it and react to the failure, such at recompute on other workers.
Driver is efficient on computation, but also introduces a disadvantage to the cluster, single point failure.
It's hard and slow to recover the driver node, and the system becomes unavailable during this time.

% subsection background_driver_less (end)


\subsection{Introduction} % (fold)
\label{sub:introduction_driver_less}

Spark Driver-less is a new model of distributed computation system based on Resilient Distributed Datasets (RDD)\cite{rdd} model.
There is no driver in the cluster, instead, only workers and clients.
They communicate with each other by broadcast.
In programming language, actor model describes a cooperation model that each object runs on its own thread,
and communicate by sending message.
Spark Driver-less uses similar idea that let each worker or client runs on its own node,
and communicate by broadcast messages.

% subsection introduction_driver_less (end)


\subsection{Terms} % (fold)
\label{sub:terms_driver_less}


\begin{description}
    \item[Broadcaster] \hfill \\
    Broadcaster broadcast multiple services inside the local network.
    By listening to a certain port, it could respond to Discoverer's searching query, and return the services list.

    \item[Discoverer] \hfill \\
    Discoverer will keep sending query to certain port to ask for a certain type of services,
    and maintain a local list of services.

    \item[Service] \hfill \\
    Service describes a service with properties.
    It is defined by \emph{type}, \emph{name}, service source \emph{IP}, and \emph{port}.
    All services use a universal unique ID (UUID) as its name.

    \item[RDD] \hfill \\
    RDD is defined as the same with the original paper.
    However, in Spark Driver-less, RDD is only used to record operation lineage to generate Partition lineage.
    It only exists in Client and never be sent to any Worker.
    Every RDD has a \emph{parent} RDD except data source RDDs, and a \emph{get} function for Partition computation.

    \item[Partition] \hfill \\
    Partition is a slice of the RDD, which will be sent to a single worker to compute.
    It has a \emph{part\_id} for the index in the RDD,
    a \emph{uuid} generated from its get function,
    a \emph{parent\_list} of dependent Partitions list,
    and a \emph{func} for computation.

    It is also a Service represents a computed partition,
    with a type of \textit{partition} and its uuid as the name.
    The computed data is called Result.

    \item[Job] \hfill \\
    Job is a Service related to a partition to be computed.
    It has a type of \textit{job}, and use the partition's uuid as its name.

    \item[Worker] \hfill \\
    Worker is a computation process on an arbitrary node.
    It has a Job Discoverer, Job Broadcaster, Partition Discoverer, and Partition Broadcaster.

    \item[Client] \hfill \\
    Client is a interactive console for user to compute in the cluster.
    All computation is lazy.
    It creates RDD lineage and transform it into Partition lineage.
    Broadcast target partitions as a Job.
\end{description}

% subsection terms_driver_less (end)


\subsection{Implementation} % (fold)
\label{sub:implementation_driver_less}

\subsubsection{Client} % (fold)
\label{ssub:client}
Client accepts user's input. It's a lazy interactive console.
User would define their RDD lineage. But until any action is called, there is no real computation at all.
It will record down the parent RDD and the function or other parameters related to the RDDs\@.
If it's a wide dependency, it will first create a Repartition RDD between itself and its parent.

When user called a action, Client will firstly create the Partition lineage based on the given RDD lineage.
Any Partition will only have the dependency links to the partitions it needs in the parent RDD\@.
If it's a narrow dependency, it will only have one dependency partition.
And if it's a wide dependency, it will depend on multiple ones.
By limiting the reference inside Partition object,
it becomes easy to dump the partition object by CloudPickle (TODO cite) without introduce any unnecessary object,
which will reduce data transferring and dump failure.

Secondly, for every partition in the final target RDD, the client will create a job for it.
And broadcast them by partitions' uuid.
Only these partitions will be created as a job,
because these partitions are the ones that the user really care about, not those intermediate results.

Finally, it will keep discovering for the finished Partitions of these jobs.
And fetch them back to local when find them available in the cluster.
Merge the results of those partitions and return final result to the user.
% subsubsection client (end)

\subsubsection{Worker} % (fold)
\label{ssub:worker}
When the Job Discoverer of a Worker finds a job broadcasted in the cluster,
it will connect back to the source and retrieve the corresponding Partition object back,
and add it to the Worker's Jobs queue.

Worker has a thread that keep trying to take the Partition object out of the Jobs queue.
After successfully taken one out, it will call the Partition object's function.
For the narrow dependency transform, like Map and Filter, it will directly call the parent partition's function.
For the wide dependency transform, like Repartition, more works will be done.

First, it will check all the dependency partitions in its own Partition Discoverer,
and save the results to the Partition objects.
If that partition is local, the discoverer will simply return the partition's result.
Otherwise, it will connect to the remote worker, fetch the result back, and cache it locally for the future needs.

Second, the worker will find all the missing parent partitions that can't be found in the cluster.
If any parent partition is missing, it will wrap them as a Job and broadcast it.
Then suspend the current job by appending it to the end of the jobs queue.
If all parent partitions are done, the worker will compute the current job, since all the dependencies are local now.
And put it into Partition Broadcaster to announce that the partition is finished in the cluster.
% subsubsection worker (end)

\subsubsection{Broadcaster} % (fold)
\label{ssub:broadcaster}
TODO: worker broadcaster is only for optimization of \emph{num\_of\_partitions}
% subsubsection broadcaster (end)
% subsection implementation_driver_less (end)


\subsection{Guarantee} % (fold)
\label{sub:guarantee}

\subsubsection{Accomplishment} % (fold)
\label{ssub:accomplishment}
TODO
% subsubsection accomplishment (end)

% subsection guarantee (end)


\subsection{Future Work} % (fold)
\label{sub:future_work_on_driver_less}
1. GEvent is not enough, true thread model is better.
% subsection future_work_on_driver_less (end)


% section spark_driver_less (end)
