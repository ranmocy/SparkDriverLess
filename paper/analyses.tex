%!TEX root = main.tex

\section{Analyses} % (fold)
\label{sec:analyses}

\subsection{Accomplishment} % (fold)
\label{sub:accomplishment}
Since the RDD lineage is a directed acyclic graph,
the partition lineage should also be a directed acyclic graph, even with wide dependencies.
This means, for any given partition in the partition lineage,
there is always a topological sort for all its dependencies to be executed sequentially.
This guarantees that any given partition could be done eventually.
% subsection accomplishment (end)

\subsection{UUID} % (fold)
\label{sub:uuid}
The key point of the whole system is the UUID of partitions.
It is generated by first dumping the partition's function into string by CloudPickle and then hash the string into an id.
The id is considered as a universal unique, but it's not guaranteed.
However, it's very rare to have a conflict.
Consider the bits of the hash is $l$, then the total hash space has $m=2^l$ numbers.
And random pick $n$ ids from it.
Then it's a pigeonhole problem\cite{pigeonhole} with $m$ and $n$.
The likelihood of having a conflict is:
$$P(x) = 1 - \frac{{(m)}_{n}}{m^{n}} = 1 - \prod\limits_{k = 1}^n\frac{m - k + 1}{m}$$
If given $l=64$ and $n=10^5$, which means 64 bits hash function and $10^5$ partitions at the same time.
Then the likelihood of having a conflict is:

\begin{align*}
1 - P(x) &= \prod\limits_{k = 1}^n\frac{m - k + 1}{m} \\
\ln(1 - P(x)) &= \ln(\prod\limits_{k = 1}^n\frac{m - k + 1}{m}) \\
&= \sum\limits_{k = 1}^n(\ln(m - k + 1) - \ln(m)) \\
&= \sum\limits_{k = 1}^{10^5}(\ln(2^{64} - k + 1) - \ln(2^{64})) \\
&= -1.39698\e{-9} \\
P(x) &= 1 - e^{-1.39698\e{-9}} \\
&= 2.71048\e{-10}
\end{align*}

The code to calculate is in Appendix~\ref{sub:uuid_analysis_python_code}.

Moreover, the bits of the hash could be arbitrary large to reach the desire level.
% subsection uuid (end)

\subsection{Cache} % (fold)
\label{sub:cache}
Anywhere that accesses the result of each partition, will cache theses results.
This could be done because of the nature of RDD\@.
RDD is a immutable and stateless data structure.
Which means that the result of a partition is independent of when and where is computed,
even with repeatedly computations.
They are cached in:
\begin{enumerate}
    \item RDD's \emph{collect} method.
    \item Client's Partition Discoverer.
    \item Worker's Partition Discoverer.
    \item Worker's Partition Broadcaster.
\end{enumerate}
% subsection cache (end)

\subsection{Data Expiration} % (fold)
\label{sub:data_expiration}
Because the UUID of the partition only relies on the function closure not the data source,
it will always the same even the data is changed.
Spark Driverless shares the same assumption that the data won't be changed during computation.
However, there are several ways to expire the partitions:
\begin{enumerate}
    \item Restart all the workers.
    This is the brute force way to do it.
    It's very cheap to restart workers, since they are stateless and no need to configure.
    This is the easiest way to guarantee expiring those partitions.
    \item Add a time-stamp to the function closure.
    So that the UUID will be changed.
    This also guarantees expiring partitions.
    \item Broadcast to tell workers to expire a certain partition.
    However, this can't guarantee to expire without any further mechanism.
\end{enumerate}
% subsection data_expiration (end)

\subsection{Lazy} % (fold)
\label{sub:lazy}
There is not any computation until any worker or client requires it.
The work flow and data flow is not planned ahead but determined just in time.
% subsection lazy (end)

\subsection{Decentralization} % (fold)
\label{sub:decentralization}
Spark Driverless is totally decentralized.
Decentralization means no single point failure.
And all nodes dynamically find each others in runtime.
So that the cluster becomes easy to maintain, since no configuration is required at all.
% subsection decentralization (end)

\subsection{Fault Tolerant} % (fold)
\label{sub:fault_tolerant}
There is no code directly related to fault tolerant.
Fault tolerant comes from the nature of the combination of
stateless dataset, lazy evaluation, and decentralization.
There is no difference between a normal evaluation and a fault recovery.
% subsection fault_tolerant (end)

% section analyses (end)
