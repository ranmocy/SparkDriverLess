%!TEX root = main.tex

\section{Analyses} % (fold)
\label{sec:analyses}

\subsection{Accomplishment} % (fold)
\label{sub:accomplishment}
Since the RDD lineage is a directed acyclic graph,
the partition lineage should also be a directed acyclic graph, even with wide dependencies.
This means, for any given partition in the partition lineage,
there is always a topological sort for all its dependencies to be executed sequentially.
This guarantees that any given partition could be done eventually.
% subsection accomplishment (end)

\subsection{UUID} % (fold)
\label{sub:uuid}
The key point of the whole system is the UUID of partitions.
It is generated by first dumping the partition's function into string by CloudPickle and then hash the string into an id.
The id is considered as a universal unique, but it's not guaranteed.
However, it's very rare to have a conflict.
Consider the bits of the hash is $l$, then the total hash space has $m=2^l$ numbers.
And random pick $n$ ids from it.
Then it's a pigeonhole problem\cite{pigeonhole} with $m$ and $n$.
The likelihood to have a conflict is:
$$P(x) = 1 - \frac{{(m)}_n}{m^n} = 1 - \prod\limits_{k = 1}^n\frac{m - k + 1}{m}$$
If given $l=64$ and $n=10^6$, the likelihood is:

\begin{align*}
1 - P(x) &= \prod\limits_{k = 1}^n\frac{m - k + 1}{m} \\
\ln(1 - P(x)) &= \ln(\prod\limits_{k = 1}^n\frac{m - k + 1}{m}) \\
&= \sum\limits_{k = 1}^n(\ln(m-k+1) - \ln(m)) \\
&= \sum\limits_{k=1}^{10^5}(\ln(2^{64}-k+1) - \ln(2^{64})) \\
&= -1.39698\e{-9} \\
P(x) &= 1 - e^{-1.39698\e{-9}} \\
&= 2.71048\e{-10}
\end{align*}

The code to calculate is in Appendix~\ref{sub:uuid_analysis_python_code}.

Moreover, the bits of the hash could be arbitrary large to reach the desire level.
% subsection uuid (end)

\subsection{Cache} % (fold)
\label{sub:cache}
Anywhere that accesses the result of each partition, will cache theses results.
This could be done because of the nature of RDD\@.
RDD is a immutable and stateless data structure.
Which means that the result of a partition is independent of when and where is computed,
even with repeatedly computations.
They are cached in:
\begin{enumerate}
    \item RDD's \emph{collect} method.
    \item Client's Partition Discoverer.
    \item Worker's Partition Discoverer.
    \item Worker's Partition Broadcaster.
\end{enumerate}
% subsection cache (end)

\subsection{Data Expiration} % (fold)
\label{sub:data_expiration}
Because the UUID of the partition only relies on the function closure not the data source,
it will always the same even the data is changed.
Spark Driverless shares the same assumption that the data won't be changed during computation.
However, there are several ways to expire the partitions:
\begin{enumerate}
    \item Restart all the workers.
    This is the brute force way to do it.
    It's very cheap to restart workers, since they are stateless and no need to configure.
    This is the easiest way to guarantee expiring those partitions.
    \item Add a time-stamp to the function closure.
    So that the UUID will be changed.
    This also guarantees expiring partitions.
    \item Broadcast
\end{enumerate}
% subsection data_expiration (end)

\subsection{Lazy} % (fold)
\label{sub:lazy}

% subsection lazy (end)

\subsection{Decentralization} % (fold)
\label{sub:decentralization}

% subsection decentralization (end)

\subsection{Fault Tolerant} % (fold)
\label{sub:fault_tolerant}
Achieve it without code about it.
% subsection fault_tolerant (end)


% section analyses (end)
